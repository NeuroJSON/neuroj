#!/bin/bash
###############################################################################
#
# Neuroimaging dataset to JSON converter (njprep)
#
# Author:  Qianqian Fang <q.fang at neu.edu>
# License: BSD-3-clause
# Version: 0.5
# Github:  https://github.com/NeuroJSON/neuroj
#
###############################################################################
#
# Functions:
#   njprep is a neuroimaging-data file to JSON converter following the general
#   rationale of the NeuroJSON project - that is to separate a dataset
#   into human-readable/searchable part and a non-searchable/binary data portion.
#
#   The human-readable part are stored in JSON and can be readily uploaded
#   to modern document-store databases to allow us to scale to processing large
#   datasets, making the data searchable, findable and almost universally parsable.
#
#   The non-searchable data are stored in binary JSON, or other formats and
#   can be stored externally while still being associated with the searchable
#   JSON data using links, URLs or stored as "attachments" to the JSON document.
#   They can be "re-united" with the searchable data on-demand to restore the
#   full dataset again.
#
#   For conversion of human-readable data files, njprep currently supports
#   .json, .tsv, .csv, and various text files (.txt/.md/.rst); for a limit set
#   of neuroimaging data files, such as .nii.gz, .snirf, njprep parse the file
#   header into JSON while storing the rest into binary files. njprep also converts
#   symbolic links to a special JSON element to maintain the linkage.
#
# Dependencies:
#
# For Linux and Mac OS:
#   jq, curl, octave, jbids (https://github.com/NeuroJSON/jbids - including 4
#   submodules under tools), libjson-xs-perl (for JSON::XS)
#
#   when using Ubuntu, all dependencies can be set up using the below commands
#      sudo apt-get install jq curl octave libjson-xs-perl
#      git clone --recursive https://github.com/NeuroJSON/neuroj.git ~/neuroj
#      echo "addpath(genpath('~/neuroj/jbids'));" >> ~/.octaverc
#      #echo 'export PATH=~/neuroj/bin:$PATH' >> ~/.bashrc           # optional
#
# For Windows: please install cygwin64 (http://cygwin.com/) or MSYS2 (http://msys2.org/)
#   please also install jq, curl, octave, jbids (https://github.com/NeuroJSON/jbids)
#
# Converting some of the data files, such as .snirf or
# .nii/.nii.gz requires octave and the jbids toolbox (including its submodules).
# Other functionalities does not require octave.
#
# Terminology:
#   "database": a collection of datasets, such as OpenNeuro, Dandi, BrainLife etc
#   "dataset" : a group of data files that are acquired for a specific study/purpose, may have multiple subjects folders
#   "subject folder" : a folder storing data obtained from a study subject (human or animal)
#   "data file": a single data file that is part of a dataset
#   "BIDS dataset": a dataset that is organized following the BIDS standard (https://bids.neuroimaging.io/)
#
# How to use:
#    # set default external link URL template
#    export NJPREP_FALLBACK_URL="https://neurojson.org/io/stat.cgi?db=openneuro&doc="
#
#    # convert all datasets in a database to JSON
#    njprep /database/root/ /output/json/root/ database_name
#
#    # convert a specific dataset "dataset_name" in a specific database to JSON
#    njprep /database/root/ /output/json/root/ database_name dataset_name
#
#    # convert a single file or subject-folder in a given database and dataset
#    njprep /database/root/ /output/json/root/ database_name dataset_name /path/to/a/file
#
###############################################################################

set -u #Treat unset variables as an error when substituting.

if [[ $# -lt 3 ]] ; then
  echo "format: njprep /input/root/folder /output/root/folder database_name <dataset_name>"
  exit
fi

if [[ $3 =~ ^https*:\/\/ ]]; then
  ATTACH_URL=$3
else
  ATTACH_URL="https://neurojson.org/io/stat.cgi?action=get&db=$3&doc="
fi

MAX_TSV=${NJPREP_MAX_TSV:-65536}
MAX_JSON=${NJPREP_MAX_JSON:-65536}
MAX_BJSON=${NJPREP_MAX_BJSON:-65536}
MAX_BVEC=${NJPREP_MAX_BVEC:-65536}
FALLBACK_URL=${NJPREP_FALLBACK_URL:-""}
USE_MATLAB=${NJPREP_MATLAB:-""}

#===============================================================================
# main function
#===============================================================================

main() {
  if [[ $# -ge 5 ]]; then
    convert_file "$1" "$2" "$4" "$5"
  elif [[ $# -ge 4 ]]; then # convert a single dataset
    convert_dataset "$1" "$2" "$4"
    [[ -d "$2/$4" ]] && bids2json "$2" "$4"
  else
    convert_database "$@"
  fi
}

#===============================================================================
# convert_database(inputroot, outputroot)
#===============================================================================

function convert_database() {
  if [[ -f "$1/dataset_description.json" ]]; then
    convert_dataset "$1" "$2" "$4"
  fi

  echo "converting database ..."

  for ds in ${1%/}/*/
  do
    local dsname=`basename ${ds%/}`
    echo "### processing $dsname"
    convert_dataset "$1" "$2" "$dsname"
  done

  echo bids2json "$2"
  [[ -d "$2" ]] && bids2json "$2"    # merge all subject .jbids file and top-level json files to a single json for a database
}

#===============================================================================
# convert_dataset(datasetroot, outputroot, dsname)
#===============================================================================

function convert_dataset() {
  local dsname=$3

  [[ -f "$2/${dsname%/}.json" ]] && return
  shopt -s globstar

  echo "converting dataset $dsname"
  rm -rf "$2/.hash/${dsname%/}/"

  for ff in ${1%/}/${dsname%/}/**
  do
    convert_file "$1" "$2" "$dsname" "$ff"
  done

  echo mergejson "$2/${dsname}"
  [[ -d "$2/${dsname}" ]] && mergejson "$2/${dsname}"   # merge subject-folder json files to a single .jbids file
}

#===============================================================================
# convert_file(inputroot, outputroot, dsname, filename)
#===============================================================================

function convert_file() {
  local dsname="$3"
  local ff="$4"
  local ds="$1/$dsname"

  echo "converting file $ff"

  fname=$(basename "$ff")
  foldername=$(dirname "$ff")

  if [[ -L "$ff" ]]; then     # a symbolic link
     relpath=`realpath --relative-to="$1" "$foldername"`
     relpath="$relpath/$fname"
  else
     relpath=`realpath --relative-to="$1" "$ff"`
  fi

  outputdir=`dirname "$2/${relpath}"`

  relpath2=`realpath --relative-to="$ds" "$foldername"`
  relpath2="$relpath2/$fname"
  pathhash=`echo -n "/${relpath2}" | md5sum | cut -d ' ' -f 1`
  filesize=$(stat -c %s "$ff")

  [[ ! -d "$outputdir" ]] && mkdir -p "$outputdir" # create output folder if not exist

  if [[ -d "$ff" ]]; then       # a folder
     return

  elif [[ -L "$ff" ]]; then     # a symbolic link
     link2json "$ff" "$ATTACH_URL${dsname%/}&file=$relpath2" > "$2/${relpath}.jbids"

  elif [[ -z "$ff" ]]; then     # empty file or all white-space
     echo "[]" > "$2/${relpath}.jbids"

  elif [[ "${ff##*.}" == "tsv" || "${ff##*.}" == "csv" ]]; then    # tsv or csv files
     if [[ "$filesize" -lt $MAX_TSV || $fname == "participants.tsv" ]]; then  # tsv smaller than 128k is converted to digest
         fsha1=`sha1sum "$ff" | cut -d ' ' -f 1`
         if [[ -f "$2/.hash/${dsname%/}/$fsha1" ]]; then
            newref=$(cat "$2/.hash/${dsname%/}/$fsha1" | sed -e 's/\./\\\\\./g' -e 's/\//./g')
            echo "{\"_DataLink_\" : \"\$.${newref}\"}" > "$2/${relpath}.jbids"
         else
            tsv2json "$ff" > "$2/${relpath}.jbids"
            [[ ! -d "$2/.hash/$dsname" ]] && mkdir -p "$2/.hash/${dsname%/}/"
            echo "${relpath2}" > "$2/.hash/${dsname%/}/$fsha1"
         fi
     else
         if [[ -n "${FALLBACK_URL}" ]]; then
             echo "{\"_DataLink_\" : \"$FALLBACK_URL/${dsname%/}/$relpath2&size=$filesize\"}" > "$2/${relpath}.jbids"
         else
             [[ ! -d "$2/.att/$dsname" ]] && mkdir -p "$2/.att/${dsname%/}/"
             tsv2json "$ff" > "$2/.att/${dsname%/}/${pathhash}.tsv.json"
             filesize=$(stat -c %s "$2/.att/${dsname%/}/${pathhash}.tsv.json")
             echo "{\"_DataLink_\" : \"$ATTACH_URL${dsname%/}&size=$filesize&file=${pathhash}.tsv.json\"}" > "$2/${relpath}.jbids"
         fi
     fi

  elif [[ "${ff##*.}" == "json" || "${ff##*.}" == "jmsh" || "${ff##*.}" == "jnii" || "${ff##*.}" == "jnirs" ]]; then   # json files
     fsha1=`sha1sum "$ff" | cut -d ' ' -f 1`
     if [[ "$filesize" -lt $MAX_JSON ]]; then  # tsv smaller than 64k is converted to digest
         if [[ -f "$2/.hash/${dsname%/}/$fsha1" ]]; then
             newref=$(cat "$2/.hash/${dsname%/}/$fsha1" | sed -e 's/\./\\\\\./g' -e 's/\//./g')
             echo "{\"_DataLink_\" : \"\$.${newref}\"}" > "$2/${relpath}"
         else
             jq -c '.' "$ff" > "$2/${relpath}"
             [[ ! -d "$2/.hash/$dsname" ]] && mkdir -p "$2/.hash/${dsname%/}/"
             echo "${relpath2}" > "$2/.hash/${dsname%/}/$fsha1"
         fi
     else
         if [[ -n "${FALLBACK_URL}" ]]; then
             echo "{\"_DataLink_\" : \"$FALLBACK_URL/${dsname%/}/$relpath2&size=$filesize\"}" > "$2/${relpath}.jbids"
         else
             [[ ! -d "$2/.att/$dsname" ]] && mkdir -p "$2/.att/${dsname%/}/"
             jq -c '.' "$ff" > "$2/.att/${dsname%/}/${pathhash}.json"
             filesize=$(stat -c %s "$2/.att/${dsname%/}/${pathhash}.json")
             echo "{\"_DataLink_\" : \"$ATTACH_URL${dsname%/}&size=$filesize&file=${pathhash}.json\"}" > "$2/${relpath}.jbids"
         fi
     fi
  elif [[ "${ff##*.}" == "bmsh" || "${ff##*.}" == "bnii" || "${ff##*.}" == "bnirs" ]]; then   # json files
     if [[ "$filesize" -lt $MAX_BJSON ]]; then  # tsv smaller than 64k is converted to digest
         python3 << EOF
import jdata as jd
data = jd.load('$ff');
jd.savet(data, '$2/${relpath}'+'.jbids', {'compression':'zlib'})
EOF
     else
         [[ ! -d "$2/.att/$dsname" ]] && mkdir -p "$2/.att/${dsname%/}/"
         cp -a "$ff" "$2/.att/${dsname%/}/${pathhash}.${ff##*.}"
         echo "{\"_DataLink_\" : \"$ATTACH_URL${dsname%/}&size=$filesize&file=${pathhash}.${ff##*.}\"}" > "$2/${relpath}.jbids"
     fi

  elif [[ "${ff##*.}" == "bval" || "${ff#*.}" == "bvec" ]]; then   # bval and bvec files
     if [[ "$filesize" -gt $MAX_BVEC ]]; then  # tsv smaller than 64k is converted to digest
         echo "{\"_DataLink_\" : \"$FALLBACK_URL/${dsname%/}/$relpath2&size=$filesize\"}" > "$2/${relpath}.jbids"
     else
         python3 << EOF
import numpy as np
import jdata as jd
data = np.genfromtxt('$ff', dtype=np.float32)
jd.savet(data, '$2/${relpath}'+'.jbids', {'compression':'zlib'})
EOF
     fi
  elif [[ "${ff##*.}" == "nii"  || "${ff#*.}" == "nii.gz" ]]; then  # nii.gz files (if not a link) - use octave+jnifty toolbox
     [[ ! -d "$2/.att/$dsname" ]] && mkdir -p "$2/.att/$dsname/"
     cmd=$(cat <<EOF
nii = nii2jnii('$ff');
nii.NIFTIHeader.SrcFilePath='${relpath}';
savebj('', nii, 'filename', '$2/.att/$dsname/${pathhash}-zlib.bnii','compression','zlib');
info=dir('$2/.att/$dsname/${pathhash}-zlib.bnii');
nii.NIFTIData = struct(encodevarname('_DataLink_'), ['${ATTACH_URL}${dsname%/}&size=' num2str(info.bytes) '&file=${pathhash}-zlib.bnii:$.NIFTIData']);
if(isfield(nii, 'NIFTIExtension'))
    nii.NIFTIExtension = struct(encodevarname('_DataLink_'), ['${ATTACH_URL}${dsname%/}&size=' num2str(info.bytes) '&file=${pathhash}-zlib.bnii:$.NIFTIExtension']);
end
savejson('', nii, 'filename', '$2/${relpath}.jnii');
EOF
)
     if [[ -n "${USE_MATLAB}" ]]; then
         ${USE_MATLAB} -nojvm -nodesktop -nosplash "$cmd;exit"
     else
         octave-cli --eval "$cmd"
     fi

  elif [[ "${ff##*.}" == "snirf" ]]; then  # snirf files (if not a link) - use octave+snirfy toolbox
     [[ ! -d "$2/.att/$dsname" ]] && mkdir -p "$2/.att/$dsname/"
     cmd=$(cat <<EOF
[d1,d2]=snirf2jbids('$ff', 'pathhash', '${pathhash}', 'compression','zlib','attachproto','${ATTACH_URL}${dsname%/}&file=');
savebj('', d2, 'filename', '$2/.att/$dsname/${pathhash}-zlib.jdb','compression','zlib');
savejson('', d1, 'filename', '$2/${relpath}.json');
EOF
)
     if [[ -n "${USE_MATLAB}" ]]; then
         echo "$cmd;exit" | ${USE_MATLAB} -nojvm -nodesktop -nosplash
     else
         octave-cli --eval "$cmd"
     fi

  elif [[ "${ff##*.}" =~ ^[Tt][Xx][Tt]$ || "${ff##*.}" == "md" || "${ff##*.}" == "rst" || "${fname%.*}" =~ ^(README|CHANGES|CITATION|LICENSE)$ ]]; then # text files
     jq -Rsa . "$ff" > "$2/${relpath}.jbids"

  elif [[ "${ff##*.}" == "png" || "${ff##*.}" == "jpg" || "${ff##*.}" == "pdf" || "${ff#*.}" == "tsv.gz" ]]; then # browser readable documents
     [[ ! -d "$2/.att/$dsname" ]] && mkdir -p "$2/.att/$dsname/"
     cp -a "$ff" "$/.att/$dsname/${pathhash}.${ff#*.}"
     pathhash=`echo -n "/${relpath2}" | md5sum | cut -d ' ' -f 1`

  else
     fileurl=`echo "$relpath2" | perl -pe 's/([^^A-Za-z0-9\-_.!~*()'"'"'])/ sprintf "%%%0x", ord $1 /eg'`
     echo "{\"_DataLink_\" : \"${ATTACH_URL}${dsname%/}&size=$filesize&file=$fileurl\"}" > "$2/${relpath}.jbids"
     echo "skipping ${ff}"
  fi
}

#===============================================================================
# run main function
#===============================================================================

main "$@"; exit
